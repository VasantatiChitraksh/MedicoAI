{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DHj3lvIfukYa",
        "outputId": "763a1e5e-1433-4793-f64a-a1f61a883792"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting speechbrain\n",
            "  Downloading speechbrain-1.0.1-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting hyperpyyaml (from speechbrain)\n",
            "  Downloading HyperPyYAML-1.2.2-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from speechbrain) (1.4.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from speechbrain) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from speechbrain) (24.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from speechbrain) (1.13.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from speechbrain) (0.2.0)\n",
            "Requirement already satisfied: torch>=1.9 in /usr/local/lib/python3.10/dist-packages (from speechbrain) (2.4.1+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (from speechbrain) (2.4.1+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from speechbrain) (4.66.5)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from speechbrain) (0.24.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->speechbrain) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->speechbrain) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->speechbrain) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->speechbrain) (3.4.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->speechbrain) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->speechbrain) (2024.6.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->speechbrain) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->speechbrain) (2.32.3)\n",
            "Collecting ruamel.yaml>=0.17.28 (from hyperpyyaml->speechbrain)\n",
            "  Downloading ruamel.yaml-0.18.6-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting ruamel.yaml.clib>=0.2.7 (from ruamel.yaml>=0.17.28->hyperpyyaml->speechbrain)\n",
            "  Downloading ruamel.yaml.clib-0.2.12-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.9->speechbrain) (3.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->speechbrain) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->speechbrain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->speechbrain) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->speechbrain) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.9->speechbrain) (1.3.0)\n",
            "Downloading speechbrain-1.0.1-py3-none-any.whl (807 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m807.2/807.2 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading HyperPyYAML-1.2.2-py3-none-any.whl (16 kB)\n",
            "Downloading ruamel.yaml-0.18.6-py3-none-any.whl (117 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.8/117.8 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ruamel.yaml.clib-0.2.12-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (722 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m722.2/722.2 kB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ruamel.yaml.clib, ruamel.yaml, hyperpyyaml, speechbrain\n",
            "Successfully installed hyperpyyaml-1.2.2 ruamel.yaml-0.18.6 ruamel.yaml.clib-0.2.12 speechbrain-1.0.1\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n",
            "Requirement already satisfied: torch==2.4.1 in /usr/local/lib/python3.10/dist-packages (from torchaudio) (2.4.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.4.1->torchaudio) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.1->torchaudio) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.4.1->torchaudio) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.4.1->torchaudio) (3.4.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.1->torchaudio) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.4.1->torchaudio) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.4.1->torchaudio) (3.0.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.4.1->torchaudio) (1.3.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install speechbrain\n",
        "!pip install torchaudio\n",
        "!pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0PBtNC0Hwia2",
        "outputId": "ca918098-827c-41a8-a8b7-71d5e5aecd02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: speechbrain in /usr/local/lib/python3.10/dist-packages (1.0.1)\n",
            "Requirement already satisfied: hyperpyyaml in /usr/local/lib/python3.10/dist-packages (from speechbrain) (1.2.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from speechbrain) (1.4.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from speechbrain) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from speechbrain) (24.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from speechbrain) (1.13.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from speechbrain) (0.2.0)\n",
            "Requirement already satisfied: torch>=1.9 in /usr/local/lib/python3.10/dist-packages (from speechbrain) (2.4.1+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (from speechbrain) (2.4.1+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from speechbrain) (4.66.5)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from speechbrain) (0.24.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->speechbrain) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->speechbrain) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->speechbrain) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->speechbrain) (3.4.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->speechbrain) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->speechbrain) (2024.6.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->speechbrain) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->speechbrain) (2.32.3)\n",
            "Requirement already satisfied: ruamel.yaml>=0.17.28 in /usr/local/lib/python3.10/dist-packages (from hyperpyyaml->speechbrain) (0.18.6)\n",
            "Requirement already satisfied: ruamel.yaml.clib>=0.2.7 in /usr/local/lib/python3.10/dist-packages (from ruamel.yaml>=0.17.28->hyperpyyaml->speechbrain) (0.2.12)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.9->speechbrain) (3.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->speechbrain) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->speechbrain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->speechbrain) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->speechbrain) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.9->speechbrain) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade speechbrain\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOkMGBfiu1CO",
        "outputId": "4d9011bb-5ac8-4ae1-e2aa-541b41687ccb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor([[0.4683, 0.6433, 0.5721, 0.5682, 0.5529, 0.5014, 0.5242, 0.5312, 0.5483,\n",
            "         0.5239, 0.5550, 0.5771, 0.6023, 0.5685, 0.5994, 0.5777, 0.8314, 0.6077,\n",
            "         0.6169, 0.5512, 0.9523, 0.5963, 0.5759, 0.5392, 0.5213, 0.5709, 0.5909,\n",
            "         0.6136, 0.5665, 0.6163, 0.4935, 0.5045, 0.7268, 0.5666, 0.6504, 0.5423,\n",
            "         0.6213, 0.5778, 0.5684, 0.5360, 0.5117, 0.5363, 0.6189, 0.5553, 0.5915,\n",
            "         0.5858, 0.5786, 0.5378, 0.5210, 0.5869, 0.5069, 0.5982, 0.6294, 0.5847,\n",
            "         0.5279, 0.5383, 0.5462, 0.5687, 0.5523, 0.7439, 0.5682, 0.5286, 0.5409,\n",
            "         0.5416, 0.6254, 0.5909, 0.5588, 0.5474, 0.6314, 0.6319, 0.6346, 0.4923,\n",
            "         0.5079, 0.5642, 0.5307, 0.5844, 0.5451, 0.5363, 0.4403, 0.6982, 0.5632,\n",
            "         0.5294, 0.5938, 0.6132, 0.5726, 0.5651, 0.5566, 0.5955, 0.5725, 0.6076,\n",
            "         0.5704, 0.4963, 0.5555, 0.4966, 0.5849, 0.5026, 0.6023, 0.5407, 0.5551,\n",
            "         0.5227, 0.5274, 0.4854, 0.5855, 0.6570, 0.5914, 0.5440, 0.5763]]), tensor([0.9523]), tensor([20]), ['en'])\n",
            "['en']\n",
            "torch.Size([1, 1, 256])\n"
          ]
        }
      ],
      "source": [
        "import torchaudio\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "from torch import tensor\n",
        "from speechbrain.inference.classifiers import EncoderClassifier\n",
        "language_id = EncoderClassifier.from_hparams(source=\"TalTechNLP/voxlingua107-epaca-tdnn\", savedir=\"tmp\")\n",
        "# Download Thai language sample from Omniglot and cvert to suitable form\n",
        "signal = language_id.load_audio(\"sample-5.mp3\")\n",
        "prediction =  language_id.classify_batch(signal)\n",
        "print(prediction)\n",
        "# The scores in the prediction[0] tensor can be interpreted as cosine scores between\n",
        "# the languages and the given utterance (i.e., the larger the better)\n",
        "# The identified language ISO code is given in prediction[3]\n",
        "print(prediction[3])\n",
        "\n",
        "# Alternatively, use the utterance embedding extractor:\n",
        "emb =  language_id.encode_batch(signal)\n",
        "print(emb.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMFv5YZHumpZ",
        "outputId": "283b5189-66eb-4aa0-c82f-0c4f9be35442"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<speechbrain.dataio.encoder.CategoricalEncoder object at 0x7b5f2fae0ca0>\n"
          ]
        }
      ],
      "source": [
        "print(language_id.hparams.label_encoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tJ9fO-4OStiT",
        "outputId": "bf2fd5f3-0559-4b46-ed38-0318f15d506f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Failed to open the input \"en.wav\" (Too many levels of symbolic links).\nException raised from get_input_format_context at /__w/audio/audio/pytorch/audio/src/libtorio/ffmpeg/stream_reader/stream_reader.cpp:42 (most recent call first):\nframe #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7b606728ef86 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)\nframe #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x7b606723dd10 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)\nframe #2: <unknown function> + 0x41ff4 (0x7b6043e80ff4 in /usr/local/lib/python3.10/dist-packages/torio/lib/libtorio_ffmpeg4.so)\nframe #3: torio::io::StreamingMediaDecoder::StreamingMediaDecoder(std::string const&, std::optional<std::string> const&, std::optional<std::map<std::string, std::string, std::less<std::string>, std::allocator<std::pair<std::string const, std::string> > > > const&) + 0x14 (0x7b6043e839f4 in /usr/local/lib/python3.10/dist-packages/torio/lib/libtorio_ffmpeg4.so)\nframe #4: <unknown function> + 0x3aa7e (0x7b5f8fe4ba7e in /usr/local/lib/python3.10/dist-packages/torio/lib/_torio_ffmpeg4.so)\nframe #5: <unknown function> + 0x32647 (0x7b5f8fe43647 in /usr/local/lib/python3.10/dist-packages/torio/lib/_torio_ffmpeg4.so)\nframe #6: <unknown function> + 0x15cb2e (0x571434c94b2e in /usr/bin/python3)\nframe #7: _PyObject_MakeTpCall + 0x25b (0x571434c8b2db in /usr/bin/python3)\nframe #8: <unknown function> + 0x16b6b0 (0x571434ca36b0 in /usr/bin/python3)\nframe #9: <unknown function> + 0x167ad7 (0x571434c9fad7 in /usr/bin/python3)\nframe #10: <unknown function> + 0x15368b (0x571434c8b68b in /usr/bin/python3)\nframe #11: <unknown function> + 0xf6bb (0x7b60756156bb in /usr/local/lib/python3.10/dist-packages/torchaudio/lib/_torchaudio.so)\nframe #12: _PyObject_MakeTpCall + 0x25b (0x571434c8b2db in /usr/bin/python3)\nframe #13: _PyEval_EvalFrameDefault + 0x6b17 (0x571434c83d27 in /usr/bin/python3)\nframe #14: _PyObject_FastCallDictTstate + 0xc4 (0x571434c8a474 in /usr/bin/python3)\nframe #15: <unknown function> + 0x1674b4 (0x571434c9f4b4 in /usr/bin/python3)\nframe #16: _PyObject_MakeTpCall + 0x1fc (0x571434c8b27c in /usr/bin/python3)\nframe #17: _PyEval_EvalFrameDefault + 0x6b17 (0x571434c83d27 in /usr/bin/python3)\nframe #18: _PyFunction_Vectorcall + 0x7c (0x571434c9542c in /usr/bin/python3)\nframe #19: _PyEval_EvalFrameDefault + 0x6bc (0x571434c7d8cc in /usr/bin/python3)\nframe #20: _PyFunction_Vectorcall + 0x7c (0x571434c9542c in /usr/bin/python3)\nframe #21: _PyEval_EvalFrameDefault + 0x613a (0x571434c8334a in /usr/bin/python3)\nframe #22: _PyFunction_Vectorcall + 0x7c (0x571434c9542c in /usr/bin/python3)\nframe #23: _PyEval_EvalFrameDefault + 0x1983 (0x571434c7eb93 in /usr/bin/python3)\nframe #24: <unknown function> + 0x16b281 (0x571434ca3281 in /usr/bin/python3)\nframe #25: _PyEval_EvalFrameDefault + 0x613a (0x571434c8334a in /usr/bin/python3)\nframe #26: <unknown function> + 0x142016 (0x571434c7a016 in /usr/bin/python3)\nframe #27: PyEval_EvalCode + 0x86 (0x571434d6f8b6 in /usr/bin/python3)\nframe #28: <unknown function> + 0x23d5fd (0x571434d755fd in /usr/bin/python3)\nframe #29: <unknown function> + 0x15d689 (0x571434c95689 in /usr/bin/python3)\nframe #30: _PyEval_EvalFrameDefault + 0x6bc (0x571434c7d8cc in /usr/bin/python3)\nframe #31: <unknown function> + 0x17a8b0 (0x571434cb28b0 in /usr/bin/python3)\nframe #32: _PyEval_EvalFrameDefault + 0x26f4 (0x571434c7f904 in /usr/bin/python3)\nframe #33: <unknown function> + 0x17a8b0 (0x571434cb28b0 in /usr/bin/python3)\nframe #34: _PyEval_EvalFrameDefault + 0x26f4 (0x571434c7f904 in /usr/bin/python3)\nframe #35: <unknown function> + 0x17a8b0 (0x571434cb28b0 in /usr/bin/python3)\nframe #36: <unknown function> + 0x257fef (0x571434d8ffef in /usr/bin/python3)\nframe #37: <unknown function> + 0x168d1a (0x571434ca0d1a in /usr/bin/python3)\nframe #38: _PyEval_EvalFrameDefault + 0x8ab (0x571434c7dabb in /usr/bin/python3)\nframe #39: _PyFunction_Vectorcall + 0x7c (0x571434c9542c in /usr/bin/python3)\nframe #40: _PyEval_EvalFrameDefault + 0x6bc (0x571434c7d8cc in /usr/bin/python3)\nframe #41: _PyFunction_Vectorcall + 0x7c (0x571434c9542c in /usr/bin/python3)\nframe #42: _PyEval_EvalFrameDefault + 0x8ab (0x571434c7dabb in /usr/bin/python3)\nframe #43: <unknown function> + 0x16b281 (0x571434ca3281 in /usr/bin/python3)\nframe #44: PyObject_Call + 0x122 (0x571434ca3f22 in /usr/bin/python3)\nframe #45: _PyEval_EvalFrameDefault + 0x285e (0x571434c7fa6e in /usr/bin/python3)\nframe #46: <unknown function> + 0x16b281 (0x571434ca3281 in /usr/bin/python3)\nframe #47: _PyEval_EvalFrameDefault + 0x1983 (0x571434c7eb93 in /usr/bin/python3)\nframe #48: <unknown function> + 0x203c75 (0x571434d3bc75 in /usr/bin/python3)\nframe #49: <unknown function> + 0x15d689 (0x571434c95689 in /usr/bin/python3)\nframe #50: <unknown function> + 0x239505 (0x571434d71505 in /usr/bin/python3)\nframe #51: <unknown function> + 0x2b5e82 (0x571434dede82 in /usr/bin/python3)\nframe #52: <unknown function> + 0x15020b (0x571434c8820b in /usr/bin/python3)\nframe #53: _PyEval_EvalFrameDefault + 0x6bc (0x571434c7d8cc in /usr/bin/python3)\nframe #54: _PyFunction_Vectorcall + 0x7c (0x571434c9542c in /usr/bin/python3)\nframe #55: _PyEval_EvalFrameDefault + 0x8ab (0x571434c7dabb in /usr/bin/python3)\nframe #56: <unknown function> + 0x203c75 (0x571434d3bc75 in /usr/bin/python3)\nframe #57: <unknown function> + 0x15d689 (0x571434c95689 in /usr/bin/python3)\nframe #58: <unknown function> + 0x239505 (0x571434d71505 in /usr/bin/python3)\nframe #59: <unknown function> + 0x2b5e82 (0x571434dede82 in /usr/bin/python3)\nframe #60: <unknown function> + 0x15020b (0x571434c8820b in /usr/bin/python3)\nframe #61: _PyEval_EvalFrameDefault + 0x6bc (0x571434c7d8cc in /usr/bin/python3)\nframe #62: <unknown function> + 0x16b281 (0x571434ca3281 in /usr/bin/python3)\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-e3bb33616b41>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msignal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlanguage_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_audio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"en.wav\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mlanguage_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassify_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/speechbrain/inference/interfaces.py\u001b[0m in \u001b[0;36mload_audio\u001b[0;34m(self, path, savedir)\u001b[0m\n\u001b[1;32m    285\u001b[0m         \u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msavedir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msavedir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m         \u001b[0msignal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchaudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannels_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudio_normalizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchaudio/_backend/utils.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size, backend)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \"\"\"\n\u001b[1;32m    204\u001b[0m         \u001b[0mbackend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdispatcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_frames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannels_first\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchaudio/_backend/ffmpeg.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size)\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0mbuffer_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4096\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m     ) -> Tuple[torch.Tensor, int]:\n\u001b[0;32m--> 297\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_audio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_frames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannels_first\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchaudio/_backend/ffmpeg.py\u001b[0m in \u001b[0;36mload_audio\u001b[0;34m(src, frame_offset, num_frames, convert, channels_first, format, buffer_size)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"read\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"vorbis\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mformat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ogg\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchaudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStreamReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m     \u001b[0msample_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_src_stream_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_audio_stream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0mfilter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_load_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_frames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torio/io/_streaming_media_decoder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, format, option, buffer_size)\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_be\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mffmpeg_ext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStreamingMediaDecoderFileObj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moption\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_be\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mffmpeg_ext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStreamingMediaDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moption\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    527\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m         \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_be\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_best_audio_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Failed to open the input \"en.wav\" (Too many levels of symbolic links).\nException raised from get_input_format_context at /__w/audio/audio/pytorch/audio/src/libtorio/ffmpeg/stream_reader/stream_reader.cpp:42 (most recent call first):\nframe #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7b606728ef86 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)\nframe #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x7b606723dd10 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)\nframe #2: <unknown function> + 0x41ff4 (0x7b6043e80ff4 in /usr/local/lib/python3.10/dist-packages/torio/lib/libtorio_ffmpeg4.so)\nframe #3: torio::io::StreamingMediaDecoder::StreamingMediaDecoder(std::string const&, std::optional<std::string> const&, std::optional<std::map<std::string, std::string, std::less<std::string>, std::allocator<std::pair<std::string const, std::string> > > > const&) + 0x14 (0x7b6043e839f4 in /usr/local/lib/python3.10/dist-packages/torio/lib/libtorio_ffmpeg4.so)\nframe #4: <unknown function> + 0x3aa7e (0x7b5f8fe4ba7e in /usr/local/lib/python3.10/dist-packages/torio/lib/_torio_ffmpeg4.so)\nframe #5: <unknown function> + 0x32647 (0x7b5f8fe43647 in /usr/local/lib/python3.10/dist-packages/torio/lib/_torio_ffmpeg4.so)\nframe #6: <unknown function> + 0x15cb2e (0x571434c94b2e in /usr/bin/python3)\nframe #7: _PyObject_MakeTpCall + 0x25b (0x571434c8b2db in /usr/bin/python3)\nframe #8: <unknown function> + 0x16b6b0 (0x571434ca36b0 in /usr/bin/python3)\nframe #9: <unknown function> + 0x167ad7 (0x571434c9fad7 in /usr/bin/python3)\nframe #10: <unknown function> + 0x15368b (0x571434c8b68b in /usr/bin/python3)\nframe #11: <unknown function> + 0xf6bb (0x7b60756156bb in /usr/local/lib/python3.10/dist-packages/torchaudio/lib/_torchaudio.so)\nframe #12: _PyObject_MakeTpCall + 0x25b (0x571434c8b2db in /usr/bin/python3)\nframe #13: _PyEval_EvalFrameDefault + 0x6b17 (0x571434c83d27 in /usr/bin/python3)\nframe #14: _PyObject_FastCallDictTstate + 0xc4 (0x571434c8a474 in /usr/bin/python3)\nframe #15: <unknown function> + 0x1674b4 (0x571434c9f4b4 in /usr/bin/python3)\nframe #16: _PyObject_MakeTpCall + 0x1fc (0x571434c8b27c in /usr/bin/python3)\nframe #17: _PyEval_EvalFrameDefault + 0x6b17 (0x571434c83d27 in /usr/bin/python3)\nframe #18: _PyFunction_Vectorcall + 0x7c (0x571434c9542c in /usr/bin/python3)\nframe #19: _PyEval_EvalFrameDefault + 0x6bc (0x571434c7d8cc in /usr/bin/python3)\nframe #20: _PyFunction_Vectorcall + 0x7c (0x571434c9542c in /usr/bin/python3)\nframe #21: _PyEval_EvalFrameDefault + 0x613a (0x571434c8334a in /usr/bin/python3)\nframe #22: _PyFunction_Vectorcall + 0x7c (0x571434c9542c in /usr/bin/python3)\nframe #23: _PyEval_EvalFrameDefault + 0x1983 (0x571434c7eb93 in /usr/bin/python3)\nframe #24: <unknown function> + 0x16b281 (0x571434ca3281 in /usr/bin/python3)\nframe #25: _PyEval_EvalFrameDefault + 0x613a (0x571434c8334a in /usr/bin/python3)\nframe #26: <unknown function> + 0x142016 (0x571434c7a016 in /usr/bin/python3)\nframe #27: PyEval_EvalCode + 0x86 (0x571434d6f8b6 in /usr/bin/python3)\nframe #28: <unknown function> + 0x23d5fd (0x571434d755fd in /usr/bin/python3)\nframe #29: <unknown function> + 0x15d689 (0x571434c95689 in /usr/bin/python3)\nframe #30: _PyEval_EvalFrameDefault + 0x6bc (0x571434c7d8cc in /usr/bin/python3)\nframe #31: <unknown function> + 0x17a8b0 (0x571434cb28b0 in /usr/bin/python3)\nframe #32: _PyEval_EvalFrameDefault + 0x26f4 (0x571434c7f904 in /usr/bin/python3)\nframe #33: <unknown function> + 0x17a8b0 (0x571434cb28b0 in /usr/bin/python3)\nframe #34: _PyEval_EvalFrameDefault + 0x26f4 (0x571434c7f904 in /usr/bin/python3)\nframe #35: <unknown function> + 0x17a8b0 (0x571434cb28b0 in /usr/bin/python3)\nframe #36: <unknown function> + 0x257fef (0x571434d8ffef in /usr/bin/python3)\nframe #37: <unknown function> + 0x168d1a (0x571434ca0d1a in /usr/bin/python3)\nframe #38: _PyEval_EvalFrameDefault + 0x8ab (0x571434c7dabb in /usr/bin/python3)\nframe #39: _PyFunction_Vectorcall + 0x7c (0x571434c9542c in /usr/bin/python3)\nframe #40: _PyEval_EvalFrameDefault + 0x6bc (0x571434c7d8cc in /usr/bin/python3)\nframe #41: _PyFunction_Vectorcall + 0x7c (0x571434c9542c in /usr/bin/python3)\nframe #42: _PyEval_EvalFrameDefault + 0x8ab (0x571434c7dabb in /usr/bin/python3)\nframe #43: <unknown function> + 0x16b281 (0x571434ca3281 in /usr/bin/python3)\nframe #44: PyObject_Call + 0x122 (0x571434ca3f22 in /usr/bin/python3)\nframe #45: _PyEval_EvalFrameDefault + 0x285e (0x571434c7fa6e in /usr/bin/python3)\nframe #46: <unknown function> + 0x16b281 (0x571434ca3281 in /usr/bin/python3)\nframe #47: _PyEval_EvalFrameDefault + 0x1983 (0x571434c7eb93 in /usr/bin/python3)\nframe #48: <unknown function> + 0x203c75 (0x571434d3bc75 in /usr/bin/python3)\nframe #49: <unknown function> + 0x15d689 (0x571434c95689 in /usr/bin/python3)\nframe #50: <unknown function> + 0x239505 (0x571434d71505 in /usr/bin/python3)\nframe #51: <unknown function> + 0x2b5e82 (0x571434dede82 in /usr/bin/python3)\nframe #52: <unknown function> + 0x15020b (0x571434c8820b in /usr/bin/python3)\nframe #53: _PyEval_EvalFrameDefault + 0x6bc (0x571434c7d8cc in /usr/bin/python3)\nframe #54: _PyFunction_Vectorcall + 0x7c (0x571434c9542c in /usr/bin/python3)\nframe #55: _PyEval_EvalFrameDefault + 0x8ab (0x571434c7dabb in /usr/bin/python3)\nframe #56: <unknown function> + 0x203c75 (0x571434d3bc75 in /usr/bin/python3)\nframe #57: <unknown function> + 0x15d689 (0x571434c95689 in /usr/bin/python3)\nframe #58: <unknown function> + 0x239505 (0x571434d71505 in /usr/bin/python3)\nframe #59: <unknown function> + 0x2b5e82 (0x571434dede82 in /usr/bin/python3)\nframe #60: <unknown function> + 0x15020b (0x571434c8820b in /usr/bin/python3)\nframe #61: _PyEval_EvalFrameDefault + 0x6bc (0x571434c7d8cc in /usr/bin/python3)\nframe #62: <unknown function> + 0x16b281 (0x571434ca3281 in /usr/bin/python3)\n"
          ]
        }
      ],
      "source": [
        "signal = language_id.load_audio(\"sample-5.\")\n",
        "prediction =  language_id.classify_batch(signal)\n",
        "\n",
        "print(prediction[3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C1OAuulaTCSO"
      },
      "outputs": [],
      "source": [
        "signal = language_id.load_audio(\"iw.wav\")\n",
        "prediction =  language_id.classify_batch(signal)\n",
        "\n",
        "print(prediction[3])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchaudio"
      ],
      "metadata": {
        "id": "bvgXiTqL9d4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "elXKq98QTG7a"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
        "\n",
        "# Load pre-trained Wav2Vec2 model and processor\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "# Function to measure inference time\n",
        "def measure_inference_time(model, input_tensor):\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    with torch.no_grad():\n",
        "        start_time = time.time()\n",
        "        _ = model(input_tensor)  # Perform inference\n",
        "        end_time = time.time()\n",
        "    return end_time - start_time\n",
        "\n",
        "# Example: Assume input_tensor is preprocessed audio tensor\n",
        "input_tensor = torch.rand(1, 16000)  # Simulate an audio input of 1-second at 16kHz\n",
        "\n",
        "# Measure for non-quantized model\n",
        "inference_time_non_quant = measure_inference_time(model, input_tensor)\n",
        "print(f\"Inference time (non-quantized): {inference_time_non_quant} seconds\")\n",
        "\n"
      ],
      "metadata": {
        "id": "AdDBo1o4A3cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install psutil"
      ],
      "metadata": {
        "id": "4zEu84ibBD3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import psutil\n",
        "import os\n",
        "\n",
        "# Function to check RAM usage\n",
        "def get_memory_usage():\n",
        "    process = psutil.Process(os.getpid())\n",
        "    mem_info = process.memory_info()\n",
        "    return mem_info.rss / (1024 ** 2)  # Return memory usage in MB\n",
        "\n",
        "# Example: Measure RAM usage before and after inference for non-quantized and quantized models\n",
        "print(f\"Initial RAM usage: {get_memory_usage()} MB\")\n",
        "\n",
        "# Inference with non-quantized model\n",
        "_ = model(input_tensor)\n",
        "print(f\"RAM usage after non-quantized model: {get_memory_usage()} MB\")\n"
      ],
      "metadata": {
        "id": "46dOqU94BGqo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U openai-whisper\n"
      ],
      "metadata": {
        "id": "7LtsoJ2UWswX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "mykm3B2MXDno"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sE8GAThlWCN2"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
        "\n",
        "# Select an audio file and read it:\n",
        "ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
        "audio_sample = ds[0][\"audio\"]\n",
        "\n",
        "# Load the Whisper model in Hugging Face format:\n",
        "processor = WhisperProcessor.from_pretrained(\"openai/whisper-tiny.en\")\n",
        "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny.en\")\n",
        "\n",
        "# Use the model and processor to transcribe the audio:\n",
        "input_features = processor(\n",
        "    audio_sample[\"array\"], sampling_rate=audio_sample[\"sampling_rate\"], return_tensors=\"pt\"\n",
        ").input_features\n",
        "\n",
        "# Generate token ids\n",
        "predicted_ids = model.generate(input_features)\n",
        "\n",
        "# Decode token ids to text\n",
        "transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n",
        "\n",
        "transcription[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "L7sGb0Twn3V3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchaudio\n",
        "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
        "\n",
        "# Select an audio file and read it:\n",
        "audio_sample, sample_rate = torchaudio.load(\"test.mp3\")\n",
        "\n",
        "# Resample the audio to 16000 Hz\n",
        "resampled_audio = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)(audio_sample)\n",
        "\n",
        "# Load the Whisper model in Hugging Face format:\n",
        "processor = WhisperProcessor.from_pretrained(\"openai/whisper-tiny.en\")\n",
        "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny.en\")\n",
        "\n",
        "# Use the model and processor to transcribe the audio:\n",
        "input_features = processor(\n",
        "    resampled_audio.squeeze().numpy(), sampling_rate=16000, return_tensors=\"pt\"\n",
        ").input_features\n",
        "\n",
        "# Generate token ids\n",
        "predicted_ids = model.generate(input_features)\n",
        "\n",
        "# Decode token ids to text\n",
        "transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n",
        "\n",
        "# Print the transcription\n",
        "print(transcription[0])\n"
      ],
      "metadata": {
        "id": "prPZXaerYb8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers huggingface_hub peft bitsandbytes"
      ],
      "metadata": {
        "id": "BGo18o5OaoyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from huggingface_hub import login, HfApi\n",
        "\n",
        "# Step 1: Load the Wav2Vec2 model and processor\n",
        "model_id = \"facebook/wav2vec2-base-960h\"\n",
        "model = Wav2Vec2ForCTC.from_pretrained(model_id)\n",
        "processor = Wav2Vec2Processor.from_pretrained(model_id)\n",
        "\n",
        "print(model.named_modules())"
      ],
      "metadata": {
        "id": "6W38sr-q9-9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iYoPAECD5jWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_modules = []\n",
        "for i in range(12):\n",
        "    target_modules.extend([\n",
        "        f\"wav2vec2.encoder.layers.{i}.attention.k_proj\",\n",
        "        f\"wav2vec2.encoder.layers.{i}.attention.v_proj\",\n",
        "        f\"wav2vec2.encoder.layers.{i}.attention.q_proj\",\n",
        "        f\"wav2vec2.encoder.layers.{i}.attention.out_proj\",\n",
        "        f\"wav2vec2.encoder.layers.{i}.feed_forward.intermediate_dense\",\n",
        "        f\"wav2vec2.encoder.layers.{i}.feed_forward.output_dense\",\n",
        "    ])\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    target_modules=target_modules,\n",
        ")\n",
        "\n",
        "\n",
        "# Step 3: Apply LoRA to the model\n",
        "lora_model = get_peft_model(model, lora_config)\n"
      ],
      "metadata": {
        "id": "B0aq1_Pp6S_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_storage=torch.bfloat16,\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "KZbVLp2G8QY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "waveform, sample_rate = torchaudio.load(\"sample-1.mp3\")\n",
        "waveform = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)(waveform)\n",
        "\n",
        "# Convert stereo to mono if necessary\n",
        "if waveform.shape[0] > 1:\n",
        "    waveform = torch.mean(waveform, dim=0)\n",
        "waveform = waveform.squeeze(0)\n",
        "\n",
        "# Tokenize the audio input\n",
        "input_values = processor(waveform, return_tensors=\"pt\", sampling_rate=16000).input_values\n",
        "\n",
        "# Perform STT\n",
        "with torch.no_grad():\n",
        "    logits = lora_model(input_values).logits\n",
        "\n",
        "# Decode the logits to text\n",
        "predicted_ids = torch.argmax(logits, dim=-1)\n",
        "transcription = processor.batch_decode(predicted_ids)\n",
        "\n",
        "print(transcription[0])"
      ],
      "metadata": {
        "id": "c-tRmtbkBhWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vYF1TcdmksQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "# Function to measure inference time\n",
        "def measure_inference_time(model, input_tensor):\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    with torch.no_grad():\n",
        "        start_time = time.time()\n",
        "        _ = model(input_tensor)  # Perform inference\n",
        "        end_time = time.time()\n",
        "    return end_time - start_time\n",
        "\n",
        "# Example: Assume input_tensor is preprocessed audio tensor\n",
        "input_tensor = torch.rand(1, 16000)  # Simulate an audio input of 1-second at 16kHz\n",
        "\n",
        "# Measure for non-quantized model\n",
        "inference_time_non_quant = measure_inference_time(lora_model, input_tensor)\n",
        "print(f\"Inference time (non-quantized): {inference_time_non_quant} seconds\")\n",
        "\n"
      ],
      "metadata": {
        "id": "h9UdGDCwB602"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import psutil\n",
        "import os\n",
        "\n",
        "# Function to check RAM usage\n",
        "def get_memory_usage():\n",
        "    process = psutil.Process(os.getpid())\n",
        "    mem_info = process.memory_info()\n",
        "    return mem_info.rss / (1024 ** 2)  # Return memory usage in MB\n",
        "\n",
        "# Example: Measure RAM usage before and after inference for non-quantized and quantized models\n",
        "print(f\"Initial RAM usage: {get_memory_usage()} MB\")\n",
        "\n",
        "# Inference with non-quantized model\n",
        "_ = lora_model(input_tensor)\n",
        "print(f\"RAM usage after non-quantized model: {get_memory_usage()} MB\")\n"
      ],
      "metadata": {
        "id": "AYmuhBrLi3M1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "login(token=\"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\")\n",
        "\n",
        "# Save LoRA model\n",
        "model_name = \"CV_2307/wav2vec2-lora-quantized\"\n",
        "lora_model.push_to_hub(model_name)\n",
        "processor.push_to_hub(model_name)\n",
        "\n",
        "print(f\"Model successfully uploaded to Hugging Face Hub: {model_name}\")"
      ],
      "metadata": {
        "id": "v-ZEbgEk9xXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oUr_kqGIe24u"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}